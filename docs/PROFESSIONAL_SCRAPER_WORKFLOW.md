# Professional Web Scraper Workflow

## ğŸ¯ Overview

An intelligent, automated web scraping system that learns website structure, generates custom scrapers, extracts data, performs analysis, and produces professional PDF reports.

## ğŸš€ Quick Start

```bash
# Run the complete workflow
python scripts/auto_scraper_workflow.py <url>

# Example
python scripts/auto_scraper_workflow.py https://example.com
```

## ğŸ“‹ Workflow Steps

The system automatically executes **5 steps**:

### 1. **HTML Structure Analysis**
- Fetches the target webpage
- Analyzes HTML structure, tags, classes, and patterns
- Identifies main content areas
- Detects repeating structures (lists, cards, articles)
- Generates scraping strategy
- **Output**: `*_analysis.json`

### 2. **Custom Scraper Generation**
- Reads the analysis results
- Auto-generates a Python scraper script
- Includes proper selectors based on learned patterns
- Adds error handling and rate limiting
- **Output**: `*_scraper.py`

### 3. **Data Extraction**
- Executes the generated scraper
- Extracts structured data from the website
- Saves results to CSV and JSON
- **Output**: `scraped_*.csv`, `scraped_*.json`

### 4. **Data Analysis**
- Loads extracted data
- Performs statistical analysis
- Generates insights
- Creates visualizations (charts and graphs)
- **Output**: `*_data_analysis.json`, `charts/*.png`

### 5. **PDF Report Generation**
- Compiles all analysis results
- Creates professional PDF report with:
  - Executive summary
  - Column-by-column analysis
  - Key insights
  - Data visualizations
- **Output**: `*_report.pdf`

## ğŸ“ Project Structure

```
F:/Scrapper/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ auto_scraper_workflow.py      # Main orchestrator
â”‚   â”œâ”€â”€ intelligent_analyzer.py       # Step 1: HTML analyzer
â”‚   â”œâ”€â”€ scraper_generator.py          # Step 2: Code generator
â”‚   â”œâ”€â”€ data_analyzer.py              # Step 4: Data analysis
â”‚   â””â”€â”€ pdf_generator.py              # Step 5: PDF creation
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ *_analysis.json               # Structure analysis
â”‚   â”œâ”€â”€ *_scraper.py                  # Generated scraper
â”‚   â”œâ”€â”€ scraped_*.csv                 # Extracted data
â”‚   â”œâ”€â”€ *_data_analysis.json          # Data insights
â”‚   â”œâ”€â”€ *_report.pdf                  # Final report
â”‚   â”œâ”€â”€ *_workflow.json               # Execution log
â”‚   â””â”€â”€ charts/                       # Visualizations
â”‚       â”œâ”€â”€ data_completeness.png
â”‚       â”œâ”€â”€ distribution_*.png
â”‚       â””â”€â”€ histogram_*.png
â””â”€â”€ .venv/                            # Python environment
```

## ğŸ› ï¸ Requirements

```bash
# Install dependencies
pip install requests beautifulsoup4 lxml pandas matplotlib seaborn reportlab
```

**Required packages:**
- `requests` - HTTP requests
- `beautifulsoup4` - HTML parsing
- `lxml` - Fast XML/HTML parser
- `pandas` - Data manipulation
- `matplotlib` - Plotting
- `seaborn` - Statistical visualizations
- `reportlab` - PDF generation

## ğŸ“– Usage Examples

### Basic Usage
```bash
python scripts/auto_scraper_workflow.py http://quotes.toscrape.com
```

### With Custom Output Directory
```bash
python scripts/auto_scraper_workflow.py https://example.com F:/CustomOutput
```

### Running Individual Steps

#### Step 1: Analyze HTML Only
```bash
python scripts/intelligent_analyzer.py <url> <output.json>
```

#### Step 2: Generate Scraper from Analysis
```bash
python scripts/scraper_generator.py <analysis.json> <scraper.py>
```

#### Step 3: Run Custom Scraper
```bash
python outputs/your_scraper.py
```

#### Step 4: Analyze Data
```bash
python scripts/data_analyzer.py <data.csv> <analysis.json>
```

#### Step 5: Generate PDF Report
```bash
python scripts/pdf_generator.py <analysis.json> <report.pdf>
```

## ğŸ¨ Features

### Intelligent HTML Analysis
- âœ… Detects main content containers
- âœ… Identifies repeating patterns (lists, cards)
- âœ… Analyzes class names and structure
- âœ… Detects pagination links
- âœ… Finds data attributes
- âœ… Analyzes internal/external links

### Dynamic Scraper Generation
- âœ… Auto-generates Python code
- âœ… Selector-based extraction
- âœ… Multiple fallback strategies
- âœ… Rate limiting (respectful scraping)
- âœ… Error handling
- âœ… CSV and JSON output

### Data Analysis
- âœ… Basic statistics (mean, median, std)
- âœ… Missing data detection
- âœ… Column type analysis
- âœ… Distribution analysis
- âœ… Insight generation
- âœ… Automated visualizations

### PDF Reports
- âœ… Professional formatting
- âœ… Cover page with metadata
- âœ… Executive summary
- âœ… Column-by-column breakdown
- âœ… Key insights section
- âœ… Embedded charts
- âœ… Auto-opens when complete

## ğŸ“Š Example Output

### Workflow Execution
```
====================================================================================================
ğŸ¤– INTELLIGENT WEB SCRAPER WORKFLOW
====================================================================================================

ğŸŒ Target URL: http://quotes.toscrape.com
ğŸ“ Output Directory: F:\Scrapper\outputs

STEP 1: ANALYZING HTML STRUCTURE âœ…
STEP 2: GENERATING CUSTOM SCRAPER âœ…
STEP 3: RUNNING SCRAPER TO EXTRACT DATA âœ…
  â†’ Extracted 14 items
STEP 4: ANALYZING SCRAPED DATA âœ…
  â†’ Generated 9 insights
  â†’ Created 5 visualizations
STEP 5: GENERATING PDF REPORT âœ…
  â†’ Report: quotes_toscrape_com_20251204_230556_report.pdf

====================================================================================================
âœ… WORKFLOW COMPLETE
====================================================================================================

ğŸ“ Files Generated (11):
   ğŸ“„ quotes_toscrape_com_20251204_230556_analysis.json (1.3 KB)
   ğŸ“„ quotes_toscrape_com_20251204_230556_scraper.py (5.5 KB)
   ğŸ“„ scraped_quotes_toscrape_com_20251204_230601.csv (6.9 KB)
   ğŸ“„ quotes_toscrape_com_20251204_230556_data_analysis.json (4.6 KB)
   ğŸ“„ data_completeness.png (33.9 KB)
   ğŸ“„ distribution_content.png (57.0 KB)
   ğŸ“„ distribution_classes.png (27.1 KB)
   ğŸ“„ distribution_title.png (23.1 KB)
   ğŸ“„ distribution_link.png (61.7 KB)
   ğŸ“„ quotes_toscrape_com_20251204_230556_report.pdf (302.1 KB)
   ğŸ“„ quotes_toscrape_com_20251204_230556_workflow.json (2.8 KB)
```

## ğŸ”§ Configuration

### Customizing the Analyzer
Edit `scripts/intelligent_analyzer.py`:
- Change user agent
- Modify timeout values
- Adjust content detection heuristics

### Customizing the Scraper Generator
Edit `scripts/scraper_generator.py`:
- Add custom extraction methods
- Modify selector strategies
- Change output formats

### Customizing Visualizations
Edit `scripts/data_analyzer.py`:
- Add custom chart types
- Modify color schemes
- Adjust chart dimensions

### Customizing PDF Reports
Edit `scripts/pdf_generator.py`:
- Change page layout
- Modify fonts and colors
- Add custom sections

## ğŸš¨ Best Practices

1. **Respect robots.txt**: Check if scraping is allowed
2. **Rate limiting**: The scraper includes 1-second delays
3. **User agent**: Identifies as a legitimate browser
4. **Error handling**: Gracefully handles failures
5. **Data validation**: Checks for missing/invalid data

## ğŸ› Troubleshooting

### Issue: No data extracted
- Check if the website requires authentication
- Verify the URL is accessible
- Check if the site uses JavaScript rendering (requires Selenium)

### Issue: Unicode/Encoding errors
- Ensure output files use UTF-8 encoding
- Check Windows console encoding settings

### Issue: PDF generation fails
- Verify reportlab is installed
- Check that chart files exist
- Ensure write permissions in output directory

### Issue: Timeout errors
- Increase timeout in analyzer/scraper
- Check network connection
- Verify target site is accessible

## ğŸ“ Workflow JSON Structure

```json
{
  "url": "http://example.com",
  "started_at": "2025-12-04 23:05:56",
  "steps": [
    {
      "name": "html_analysis",
      "status": "success",
      "timestamp": "2025-12-04 23:05:58"
    },
    ...
  ],
  "files_generated": [...]
}
```

## ğŸ¯ Use Cases

- **Market Research**: Scrape competitor data
- **Price Monitoring**: Track product prices
- **Content Aggregation**: Collect articles/news
- **Data Collection**: Research datasets
- **SEO Analysis**: Analyze website structure
- **Lead Generation**: Collect contact information

## ğŸ“„ License

This project is part of the Scrapper repository.

## ğŸ¤ Contributing

Feel free to enhance the workflow:
- Add new analysis features
- Improve pattern detection
- Create custom visualization types
- Enhance PDF report layouts

## ğŸ“ Support

For issues or questions, check the generated `*_workflow.json` file for detailed execution logs.

---

**Note**: Always comply with website Terms of Service and robots.txt when scraping.
