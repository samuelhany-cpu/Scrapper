"""
BILINGUAL TELEGRAM BOT - English & Arabic Support
Updated to support both English and Arabic languages
"""
import os
import asyncio
from datetime import datetime
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes, CallbackQueryHandler
from telegram.constants import ParseMode

from src.config import Config
from src.logger import ScraperLogger
from src.adaptive_scraper import AdaptiveSmartScraper
from src.report_generator import ReportGenerator
from src.language import LanguageSupport, t

# Bot Configuration
BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN', 'YOUR_BOT_TOKEN_HERE')

class BilingualScraperBot:
    def __init__(self):
        self.active_tasks = {}
        self.user_languages = {}  # Store user language preferences
        Config.ensure_directories()
    
    def get_user_language(self, update: Update):
        """Get user's preferred language - auto-detect from Telegram or message"""
        user_id = update.effective_user.id
        
        # Check stored preference
        if user_id in self.user_languages:
            return self.user_languages[user_id]
        
        # Try Telegram language settings
        if update.effective_user.language_code:
            if update.effective_user.language_code.startswith('ar'):
                self.user_languages[user_id] = 'ar'
                return 'ar'
        
        # Detect from message text
        message = update.message or update.edited_message
        if message and message.text:
            detected = LanguageSupport.detect_language(message.text)
            self.user_languages[user_id] = detected
            return detected
        
        # Default to English
        self.user_languages[user_id] = 'en'
        return 'en'
    
    def set_user_language(self, user_id, lang):
        """Set user's language preference"""
        self.user_languages[user_id] = lang
    
    async def lang_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle /lang command - switch language"""
        message = update.message or update.edited_message
        if not message:
            return
        
        user_id = update.effective_user.id
        current_lang = self.get_user_language(update)
        
        # Toggle language
        new_lang = 'ar' if current_lang == 'en' else 'en'
        self.set_user_language(user_id, new_lang)
        
        if new_lang == 'ar':
            response = """
âœ… **ØªÙ… ØªØºÙŠÙŠØ± Ø§Ù„Ù„ØºØ© Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©**

Ø§Ù„Ø¢Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø³ØªÙƒÙˆÙ† Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ù„ØºØ© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ØŒ Ø§Ø³ØªØ®Ø¯Ù…: /lang
            """
        else:
            response = """
âœ… **Language changed to English**

All messages will now be in English.

To change language again, use: /lang
            """
        
        await message.reply_text(response, parse_mode=ParseMode.MARKDOWN)
    
    async def start_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle /start command"""
        message = update.message or update.edited_message
        if not message:
            return
        
        lang = self.get_user_language(update)
        
        if lang == 'ar':
            welcome = """
ğŸ¤– **Ø¨ÙˆØª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø§Ù…Ù„**

Ù…Ø±Ø­Ø¨Ø§Ù‹! ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø£ÙŠ Ù…ÙˆÙ‚Ø¹ ÙˆÙŠØ¨ Ù„Ùƒ.

**ğŸš€ ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:**
ÙÙ‚Ø· Ø£Ø±Ø³Ù„ Ù„ÙŠ Ø£ÙŠ Ø±Ø§Ø¨Ø· - Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„Ù„Ø£ÙˆØ§Ù…Ø±!

**Ø£Ù…Ø«Ù„Ø©:**
`https://example.com`
`https://twitter.com/username/status/123`
`https://ar.wikipedia.org/wiki/Ø§Ø³ØªØ®Ø±Ø§Ø¬_Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª`

**âœ¨ Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:**
â€¢ ğŸŒ Ù…Ø³ØªØ®Ø±Ø¬ ÙˆÙŠØ¨ Ø´Ø§Ù…Ù„ (Ø£ÙŠ Ù…ÙˆÙ‚Ø¹)
â€¢ ğŸ¦ Ù…ÙÙ†Ø²ÙÙ‘Ù„ ÙˆØ³Ø§Ø¦Ø· ØªÙˆÙŠØªØ± (Ù…ØµØ§Ø¯Ù‚)
â€¢ ğŸ¤– Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (Gemini Ù…Ø¬Ø§Ù†Ø§Ù‹)
â€¢ ğŸ“Š ØªØµØ¯ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª CSV
â€¢ ğŸ“„ ØªÙ‚Ø§Ø±ÙŠØ± PDF
â€¢ ğŸ“‹ Ø³Ø¬Ù„Ø§Øª Ù…ÙØµÙ„Ø©
â€¢ ğŸŒ ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©

**ğŸ“‹ Ø£ÙˆØ§Ù…Ø± Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©:**
/start - Ø¹Ø±Ø¶ Ù‡Ø°Ù‡ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
/scrape <Ø±Ø§Ø¨Ø·> - Ø·Ø±ÙŠÙ‚Ø© Ø¨Ø¯ÙŠÙ„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
/help - Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…ÙØµÙ„Ø©
/lang - ØªØºÙŠÙŠØ± Ø§Ù„Ù„ØºØ© (English/Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)
/stats - Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª

**ğŸ¯ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…:**
âœ… Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±ØŒ Ø§Ù„Ù…Ø¯ÙˆÙ†Ø§ØªØŒ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§
âœ… Ø§Ù„ØªØ¬Ø§Ø±Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©ØŒ ØµÙØ­Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
âœ… ØµÙˆØ± ÙˆÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª ØªÙˆÙŠØªØ±/X
âœ… Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ØŒ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…ØŒ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
âœ… Ø£ÙŠ Ù…ÙˆÙ‚Ø¹ Ø¹Ø§Ù…!

Ø¬Ø±Ø¨Ù‡ Ø§Ù„Ø¢Ù†! ÙÙ‚Ø· Ø§Ù„ØµÙ‚ Ø±Ø§Ø¨Ø· ğŸ‘‡
            """
        else:
            welcome = """
ğŸ¤– **Universal Web Scraper Bot**

Welcome! I can scrape any website and extract data for you.

**ğŸš€ How to Use:**
Just send me any URL - no commands needed!

**Examples:**
`https://example.com`
`https://twitter.com/username/status/123`
`https://en.wikipedia.org/wiki/Web_scraping`

**âœ¨ Features:**
â€¢ ğŸŒ Universal web scraper (any site)
â€¢ ğŸ¦ Twitter media downloader (authenticated)
â€¢ ğŸ¤– AI-powered extraction (FREE Gemini)
â€¢ ğŸ“Š CSV data export
â€¢ ğŸ“„ PDF reports
â€¢ ğŸ“‹ Detailed logs
â€¢ ğŸŒ Supports English & Arabic

**ğŸ“‹ Optional Commands:**
/start - Show this message
/scrape <URL> - Alternative way to scrape
/help - Detailed help
/lang - Change language (English/Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)
/stats - View statistics

**ğŸ¯ Supported:**
âœ… News sites, blogs, Wikipedia
âœ… E-commerce, product pages
âœ… Twitter/X images & videos
âœ… Tables, lists, articles
âœ… Any public website!

Try it now! Just paste a URL ğŸ‘‡
            """
        
        await message.reply_text(welcome, parse_mode=ParseMode.MARKDOWN)
    
    async def help_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle /help command"""
        message = update.message or update.edited_message
        if not message:
            return
        
        lang = self.get_user_language(update)
        
        if lang == 'ar':
            help_text = """
ğŸ“– **ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¨ÙˆØª**

**âš¡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© (Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§):**
ÙÙ‚Ø· Ø§Ù„ØµÙ‚ Ø£ÙŠ Ø±Ø§Ø¨Ø· - Ø§Ù„Ø¨ÙˆØª ÙŠÙƒØªØ´Ù ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ÙˆÙŠØ³ØªØ®Ø±Ø¬!
`https://example.com`

**ğŸ“‹ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ÙˆØ§Ù…Ø± (Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©):**
`/scrape https://example.com`

**ğŸ“Š Ù…Ø§ Ø³ØªØ­ØµÙ„ Ø¹Ù„ÙŠÙ‡:**
âœ… Ù…Ù„Ù CSV Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
âœ… ØªÙ‚Ø±ÙŠØ± PDF Ù…Ø¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
âœ… Ù…Ù„Ù Ø³Ø¬Ù„ Ù…ÙØµÙ„
âœ… Ù„ØªÙˆÙŠØªØ±: ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª!

**ğŸ¦ Ù…Ù…ÙŠØ²Ø§Øª Ø®Ø§ØµØ© Ù„ØªÙˆÙŠØªØ±/X:**
â€¢ ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØµØ§Ø¯Ù‚ (ÙŠØ³ØªØ®Ø¯Ù… Ù…Ù„ÙØ§Øª ØªØ¹Ø±ÙŠÙ Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù…ØªØµÙØ­)
â€¢ ğŸ–¼ ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ±
â€¢ ğŸ¥ ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª
â€¢ ğŸ“± ÙŠØ¹Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© ÙˆØ§Ù„Ø®Ø§ØµØ© (Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØªØ§Ø¨Ø¹Ù‡Ø§)
â€¢ ğŸš« Ø¨Ù„Ø§ Ø­Ø¯ÙˆØ¯ API!

**ğŸ¯ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:**
â€¢ Ø§Ù„ØªØ¬Ø§Ø±Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© (Ø£Ù…Ø§Ø²ÙˆÙ†ØŒ Ø¥ÙŠØ¨Ø§ÙŠØŒ Ø¥Ù„Ø®.)
â€¢ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙˆØ§Ù„Ù…Ø¯ÙˆÙ†Ø§Øª
â€¢ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ ÙˆØ§Ù„ØªÙˆØ«ÙŠÙ‚
â€¢ ÙˆØ³Ø§Ø¦Ø· ØªÙˆÙŠØªØ±/X (ØµÙˆØ±/ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª)
â€¢ Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙˆØ§Ù„Ø¬Ø¯Ø§ÙˆÙ„
â€¢ Ø£ÙŠ Ù…ÙˆÙ‚Ø¹ Ø¹Ø§Ù…!

**ğŸ¤– Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ©:**
â€¢ ÙƒØ´Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù†ÙˆØ¹ Ø§Ù„Ù…ÙˆÙ‚Ø¹
â€¢ ØªÙƒÙŠÙŠÙ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
â€¢ ÙŠØ³ØªØ®Ø¯Ù… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø¬Ø§Ù†ÙŠ (Gemini)
â€¢ ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…ÙˆØ§Ù‚Ø¹ JavaScript
â€¢ ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ØŒ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…ØŒ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª

**âš¡ Ø£Ù…Ø«Ù„Ø©:**
`https://ar.wikipedia.org/wiki/Ø§Ø³ØªØ®Ø±Ø§Ø¬_Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª`
`https://twitter.com/username/status/123`
`https://quotes.toscrape.com`

**ğŸŒ ØªØºÙŠÙŠØ± Ø§Ù„Ù„ØºØ©:**
Ø§Ø³ØªØ®Ø¯Ù… `/lang` Ù„Ù„ØªØ¨Ø¯ÙŠÙ„ Ø¨ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©

**â“ ØªØ­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø©ØŸ**
ÙÙ‚Ø· Ø£Ø±Ø³Ù„ Ø±Ø§Ø¨Ø· ÙˆØ³Ø£Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ùƒ!
            """
        else:
            help_text = """
ğŸ“– **How to Use This Bot**

**âš¡ Quick Method (Recommended):**
Just paste any URL - bot auto-detects and scrapes!
`https://example.com`

**ğŸ“‹ Command Method (Optional):**
`/scrape https://example.com`

**ğŸ“Š What You Get:**
âœ… CSV file with extracted data
âœ… PDF report with statistics
âœ… Detailed log file
âœ… For Twitter: Images & Videos downloaded!

**ğŸ¦ Twitter/X Special Features:**
â€¢ ğŸ” Authenticated scraping (uses browser cookies)
â€¢ ğŸ–¼ Downloads all images
â€¢ ğŸ¥ Downloads all videos
â€¢ ğŸ“± Works with public & private tweets (if you follow them)
â€¢ ğŸš« No API limits!

**ğŸ¯ Supported Sites:**
â€¢ E-commerce (Amazon, eBay, etc.)
â€¢ News websites & Blogs
â€¢ Wikipedia & Documentation
â€¢ Twitter/X media (images/videos)
â€¢ Product listings & Tables
â€¢ Any public website!

**ğŸ¤– Smart Features:**
â€¢ Auto-detects website type
â€¢ Adapts extraction strategy
â€¢ Uses FREE Gemini AI
â€¢ Handles JavaScript sites
â€¢ Extracts tables, lists, articles

**âš¡ Examples:**
`https://en.wikipedia.org/wiki/Web_scraping`
`https://twitter.com/username/status/123`
`https://quotes.toscrape.com`

**ğŸŒ Change Language:**
Use `/lang` to switch between English and Arabic

**â“ Need Help?**
Just send a URL and I'll scrape it for you!
            """
        
        await message.reply_text(help_text, parse_mode=ParseMode.MARKDOWN)
    
    async def scrape_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle /scrape command"""
        message = update.message or update.edited_message
        if not message:
            return
        
        lang = self.get_user_language(update)
        
        if not context.args:
            if lang == 'ar':
                await message.reply_text(
                    "âŒ ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ø±Ø§Ø¨Ø·!\n\nØ§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: `/scrape https://example.com`",
                    parse_mode=ParseMode.MARKDOWN
                )
            else:
                await message.reply_text(
                    "âŒ Please provide a URL!\n\nUsage: `/scrape https://example.com`",
                    parse_mode=ParseMode.MARKDOWN
                )
            return
        
        url = context.args[0]
        await self.process_scraping(update, url)
    
    async def handle_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle plain text messages (URLs)"""
        message = update.message or update.edited_message
        if not message:
            return
        
        text = message.text.strip()
        lang = self.get_user_language(update)
        
        # Check if it's a URL
        if text.startswith('http://') or text.startswith('https://'):
            await self.process_scraping(update, text)
        else:
            if lang == 'ar':
                await message.reply_text(
                    "â“ ÙŠØ±Ø¬Ù‰ Ø¥Ø±Ø³Ø§Ù„ Ø±Ø§Ø¨Ø· ØµØ§Ù„Ø­ ÙŠØ¨Ø¯Ø£ Ø¨Ù€ http:// Ø£Ùˆ https://\n\n"
                    "Ù…Ø«Ø§Ù„: `https://example.com`\n\n"
                    "Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… /help Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.",
                    parse_mode=ParseMode.MARKDOWN
                )
            else:
                await message.reply_text(
                    "â“ Please send a valid URL starting with http:// or https://\n\n"
                    "Example: `https://example.com`\n\n"
                    "Or use /help for more information.",
                    parse_mode=ParseMode.MARKDOWN
                )
    
    async def process_scraping(self, update: Update, url: str):
        """Process scraping request with bilingual support"""
        message = update.message or update.edited_message
        if not message:
            return
        
        user_id = update.effective_user.id
        username = update.effective_user.username or update.effective_user.first_name
        lang = self.get_user_language(update)
        
        # Check if Twitter URL
        is_twitter = 'twitter.com' in url or 'x.com' in url
        
        # Bilingual initial message
        if lang == 'ar':
            scraper_type = "ğŸ¦ Ù…Ø³ØªØ®Ø±Ø¬ ÙˆØ³Ø§Ø¦Ø· ØªÙˆÙŠØªØ±" if is_twitter else "ğŸŒ Ù…Ø³ØªØ®Ø±Ø¬ Ø´Ø§Ù…Ù„"
            status_message = await message.reply_text(
                f"ğŸš€ **Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬...**\n\n"
                f"ğŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: `{url}`\n"
                f"ğŸ‘¤ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {username}\n"
                f"ğŸ”§ Ø§Ù„ÙˆØ¶Ø¹: {scraper_type}\n\n"
                f"â³ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±...",
                parse_mode=ParseMode.MARKDOWN
            )
        else:
            scraper_type = "ğŸ¦ Twitter Media Scraper" if is_twitter else "ğŸŒ Universal Scraper"
            status_message = await message.reply_text(
                f"ğŸš€ **Starting scrape...**\n\n"
                f"ğŸ”— URL: `{url}`\n"
                f"ğŸ‘¤ User: {username}\n"
                f"ğŸ”§ Mode: {scraper_type}\n\n"
                f"â³ Please wait...",
                parse_mode=ParseMode.MARKDOWN
            )
        
        try:
            session_id = f"{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            logger = ScraperLogger(session_id)
            
            # Use Twitter scraper for Twitter URLs
            if is_twitter:
                await self.process_twitter_scraping_bilingual(update, url, status_message, logger, lang)
                return
            
            # Update: Initializing
            if lang == 'ar':
                await status_message.edit_text(
                    f"ğŸ”§ **ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬...**\n\n"
                    f"ğŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: `{url}`\n"
                    f"ğŸ¤– Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ + Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ØªÙƒÙŠÙÙŠØ©\n\n"
                    f"â³ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©...",
                    parse_mode=ParseMode.MARKDOWN
                )
            else:
                await status_message.edit_text(
                    f"ğŸ”§ **Initializing scraper...**\n\n"
                    f"ğŸ”— URL: `{url}`\n"
                    f"ğŸ¤– Using AI + Adaptive Strategy\n\n"
                    f"â³ Loading page...",
                    parse_mode=ParseMode.MARKDOWN
                )
            
            # Create scraper
            scraper = AdaptiveSmartScraper(logger)
            
            # Update: Scraping
            if lang == 'ar':
                await status_message.edit_text(
                    f"ğŸ” **Ø¬Ø§Ø±Ù Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬...**\n\n"
                    f"ğŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: `{url}`\n"
                    f"ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ©...\n"
                    f"ğŸ¤– Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...\n\n"
                    f"â³ Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ 30-60 Ø«Ø§Ù†ÙŠØ©...",
                    parse_mode=ParseMode.MARKDOWN
                )
            else:
                await status_message.edit_text(
                    f"ğŸ” **Scraping in progress...**\n\n"
                    f"ğŸ”— URL: `{url}`\n"
                    f"ğŸ“Š Analyzing structure...\n"
                    f"ğŸ¤– Extracting data...\n\n"
                    f"â³ This may take 30-60 seconds...",
                    parse_mode=ParseMode.MARKDOWN
                )
            
            # Run scraping
            success = scraper.scrape_url(url)
            
            if not success:
                if lang == 'ar':
                    error_msg = f"âŒ **ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬!**\n\nğŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: `{url}`\n\n"
                    error_msg += "**Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©:**\n"
                    error_msg += "â€¢ ØªÙ†Ø³ÙŠÙ‚ Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­\n"
                    error_msg += "â€¢ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ø­Ø¸ÙˆØ± Ø¨ÙˆØ§Ø³Ø·Ø© Ø¬Ø¯Ø§Ø± Ø§Ù„Ø­Ù…Ø§ÙŠØ©\n"
                    error_msg += "â€¢ Ø§Ù„Ø®Ø§Ø¯Ù… Ù„Ø§ ÙŠØ³ØªØ¬ÙŠØ¨\n"
                    error_msg += "â€¢ Ø­Ù…Ø§ÙŠØ© Ø¶Ø¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬\n\n"
                    error_msg += "ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
                else:
                    error_msg = f"âŒ **Scraping failed!**\n\nğŸ”— URL: `{url}`\n\n"
                    error_msg += "**Possible reasons:**\n"
                    error_msg += "â€¢ Invalid URL format\n"
                    error_msg += "â€¢ Site blocked by firewall\n"
                    error_msg += "â€¢ Server not responding\n"
                    error_msg += "â€¢ Anti-scraping protection\n\n"
                    error_msg += "Please check the URL and try again."
                
                await status_message.edit_text(error_msg, parse_mode=ParseMode.MARKDOWN)
                return
            
            # Success - process results
            data = scraper.get_data()
            metadata = scraper.get_metadata()
            structure_info = scraper.get_structure_analysis()
            csv_file = scraper.save_to_csv()
            report_gen = ReportGenerator(logger, scraper)
            report_file = report_gen.generate_report(csv_file)
            stats = logger.get_stats()
            log_file = logger.get_log_file()
            
            # Create summary
            patterns = structure_info['structure'].get('patterns', [])
            strategy = structure_info['strategy'].get('type', 'general')
            
            if lang == 'ar':
                summary = f"""
âœ… **Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬!**

ğŸ”— **Ø§Ù„Ø±Ø§Ø¨Ø·:** `{url}`
ğŸ“Š **Ø§Ù„Ù†Ø·Ø§Ù‚:** {metadata.get('domain', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')}
ğŸ“„ **Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:** {metadata.get('title', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')[:50]}...

ğŸ“ˆ **Ø§Ù„Ù†ØªØ§Ø¦Ø¬:**
â€¢ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {len(data)}
â€¢ Ø§Ù„Ù…Ø¯Ø©: {stats['duration']:.2f} Ø«Ø§Ù†ÙŠØ©
â€¢ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {stats['pages_scraped']}

ğŸ§  **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:**
â€¢ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {', '.join(patterns) if patterns else 'ØµÙØ­Ø© ÙˆÙŠØ¨ Ø¹Ø§Ù…Ø©'}
â€¢ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {strategy}
â€¢ Ù…Ø²ÙˆØ¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {'Gemini (Ù…Ø¬Ø§Ù†Ø§Ù‹)' if Config.GEMINI_API_KEY else 'ØªÙ‚Ù„ÙŠØ¯ÙŠ'}

ğŸ“¦ **Ø§Ù„Ù…Ù„ÙØ§Øª:**
"""
            else:
                summary = f"""
âœ… **Scraping Complete!**

ğŸ”— **URL:** `{url}`
ğŸ“Š **Domain:** {metadata.get('domain', 'N/A')}
ğŸ“„ **Title:** {metadata.get('title', 'N/A')[:50]}...

ğŸ“ˆ **Results:**
â€¢ Items extracted: {len(data)}
â€¢ Duration: {stats['duration']:.2f} seconds
â€¢ Pages scraped: {stats['pages_scraped']}

ğŸ§  **AI Analysis:**
â€¢ Detected patterns: {', '.join(patterns) if patterns else 'general webpage'}
â€¢ Strategy used: {strategy}
â€¢ AI provider: {'Gemini (FREE)' if Config.GEMINI_API_KEY else 'Traditional'}

ğŸ“¦ **Files:**
"""
            
            await status_message.edit_text(summary, parse_mode=ParseMode.MARKDOWN)
            
            # Send files
            caption_csv = "ğŸ“Š **Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª CSV**\nØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ø¨ØµÙŠØºØ© Ø¬Ø¯ÙˆÙ„" if lang == 'ar' else "ğŸ“Š **CSV Data File**\nExtracted data in spreadsheet format"
            caption_pdf = "ğŸ“„ **ØªÙ‚Ø±ÙŠØ± PDF**\nØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ ÙˆØ¥Ø­ØµØ§Ø¦ÙŠØ§Øª" if lang == 'ar' else "ğŸ“„ **PDF Report**\nDetailed analysis and statistics"
            caption_log = "ğŸ“‹ **Ù…Ù„Ù Ø§Ù„Ø³Ø¬Ù„**\nØ³Ø¬Ù„ Ù†Ø´Ø§Ø· Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØµÙ„" if lang == 'ar' else "ğŸ“‹ **Log File**\nDetailed scraping activity log"
            
            if csv_file and os.path.exists(csv_file):
                with open(csv_file, 'rb') as f:
                    await message.reply_document(document=f, filename=os.path.basename(csv_file), caption=caption_csv)
            
            if report_file and os.path.exists(report_file):
                with open(report_file, 'rb') as f:
                    await message.reply_document(document=f, filename=os.path.basename(report_file), caption=caption_pdf)
            
            if log_file and os.path.exists(log_file):
                with open(log_file, 'rb') as f:
                    await message.reply_document(document=f, filename=os.path.basename(log_file), caption=caption_log)
            
            # Final message
            final_msg = "ğŸ‰ **ØªÙ…!** Ù…Ø§Ø°Ø§ ØªØ±ÙŠØ¯ Ø£Ù† ØªÙØ¹Ù„ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ?" if lang == 'ar' else "ğŸ‰ **All done!** What would you like to do next?"
            await message.reply_text(final_msg, parse_mode=ParseMode.MARKDOWN)
            
        except Exception as e:
            if lang == 'ar':
                error_msg = f"âŒ **Ø­Ø¯Ø« Ø®Ø·Ø£!**\n\n`{str(e)}`\n\nÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø£Ùˆ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¯Ø¹Ù…."
            else:
                error_msg = f"âŒ **Error occurred!**\n\n`{str(e)}`\n\nPlease try again or contact support."
            await status_message.edit_text(error_msg, parse_mode=ParseMode.MARKDOWN)
    
    async def process_twitter_scraping_bilingual(self, update: Update, url: str, status_message, logger, lang):
        """Process Twitter scraping with bilingual support"""
        message = update.message or update.edited_message
        
        try:
            # Update status
            if lang == 'ar':
                await status_message.edit_text(
                    f"ğŸ¦ **Ù…Ø³ØªØ®Ø±Ø¬ ÙˆØ³Ø§Ø¦Ø· ØªÙˆÙŠØªØ± (Ù…ØµØ§Ø¯Ù‚)**\n\n"
                    f"ğŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: `{url}`\n\n"
                    f"ğŸ” Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„ÙØ§Øª ØªØ¹Ø±ÙŠÙ Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ù…ØªØµÙØ­ Ù„Ù„Ù…ØµØ§Ø¯Ù‚Ø©...\n"
                    f"ğŸ“¡ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ ØªÙˆÙŠØªØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù„Ø³ØªÙƒ...\n"
                    f"â³ Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ 30-60 Ø«Ø§Ù†ÙŠØ©...",
                    parse_mode=ParseMode.MARKDOWN
                )
            else:
                await status_message.edit_text(
                    f"ğŸ¦ **Twitter Media Scraper (AUTHENTICATED)**\n\n"
                    f"ğŸ”— URL: `{url}`\n\n"
                    f"ğŸ” Using browser cookies for authentication...\n"
                    f"ğŸ“¡ Accessing Twitter with your session...\n"
                    f"â³ This may take 30-60 seconds...",
                    parse_mode=ParseMode.MARKDOWN
                )
            
            # Create Twitter scraper
            logger.info("ğŸ¯ Using Gallery-DL with Firefox cookies (authenticated)")
            twitter_scraper = TwitterGalleryDLScraperAuth(logger, use_browser='firefox')
            media_items = twitter_scraper.scrape_twitter_media(url, download=True)
            
            if not media_items:
                if lang == 'ar':
                    error_msg = (
                        f"âŒ **Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙˆØ³Ø§Ø¦Ø·!**\n\n"
                        f"ğŸ”— Ø§Ù„Ø±Ø§Ø¨Ø·: `{url}`\n\n"
                        f"**Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©:**\n"
                        f"â€¢ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØµÙˆØ±/ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª\n"
                        f"â€¢ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ù…Ø­Ø°ÙˆÙØ© Ø£Ùˆ Ø®Ø§ØµØ©\n"
                        f"â€¢ ÙØ´Ù„Øª Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø© (Ø§Ù†ØªÙ‡Øª ØµÙ„Ø§Ø­ÙŠØ© Ù…Ù„ÙØ§Øª ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø·)\n"
                        f"â€¢ ØªÙˆÙŠØªØ± Ø­Ø¸Ø± Ø§Ù„Ø·Ù„Ø¨\n\n"
                        f"ğŸ’¡ **Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø©:**\n"
                        f"1. Ø§ÙØªØ­ ÙØ§ÙŠØ±ÙÙˆÙƒØ³ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø§Ø¯Ù…\n"
                        f"2. Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ ØªÙˆÙŠØªØ± (twitter.com)\n"
                        f"3. Ø£Ø¨Ù‚ ÙØ§ÙŠØ±ÙÙˆÙƒØ³ Ù…ÙØªÙˆØ­Ø§Ù‹\n"
                        f"4. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰\n\n"
                        f"Ø£Ùˆ Ø±Ø§Ø¬Ø¹ `TWITTER_COOKIES_GUIDE.md` Ù„Ù„Ø¨Ø¯Ø§Ø¦Ù„"
                    )
                else:
                    error_msg = (
                        f"âŒ **No media found!**\n\n"
                        f"ğŸ”— URL: `{url}`\n\n"
                        f"**Possible reasons:**\n"
                        f"â€¢ Tweet has no images/videos\n"
                        f"â€¢ Tweet is deleted or private\n"
                        f"â€¢ Authentication failed (cookies expired)\n"
                        f"â€¢ Twitter blocked the request\n\n"
                        f"ğŸ’¡ **To fix authentication:**\n"
                        f"1. Open Firefox on the server\n"
                        f"2. Log in to Twitter (twitter.com)\n"
                        f"3. Keep Firefox open\n"
                        f"4. Try again\n\n"
                        f"Or check `TWITTER_COOKIES_GUIDE.md` for alternatives"
                    )
                
                await status_message.edit_text(error_msg, parse_mode=ParseMode.MARKDOWN)
                
                if hasattr(twitter_scraper, 'close'):
                    twitter_scraper.close()
                return
            
            # Success - send media
            csv_file = twitter_scraper.save_to_csv()
            download_dir = twitter_scraper.get_download_directory()
            
            images = [m for m in media_items if m['type'] == 'image']
            videos = [m for m in media_items if m['type'] == 'video']
            
            if lang == 'ar':
                summary = (
                    f"ğŸ‰ **ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ³Ø§Ø¦Ø· ØªÙˆÙŠØªØ±!**\n\n"
                    f"ğŸ“Š **Ø§Ù„Ù†ØªØ§Ø¦Ø¬:**\n"
                    f"ğŸ–¼ Ø§Ù„ØµÙˆØ±: {len(images)}\n"
                    f"ğŸ¥ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª: {len(videos)}\n"
                    f"ğŸ“¦ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹: {len(media_items)}\n\n"
                    f"ğŸ“‚ ØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ù„Ù‰:\n`{download_dir}`\n\n"
                    f"ğŸ“‹ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª..."
                )
            else:
                summary = (
                    f"ğŸ‰ **Twitter Media Extracted!**\n\n"
                    f"ğŸ“Š **Results:**\n"
                    f"ğŸ–¼ Images: {len(images)}\n"
                    f"ğŸ¥ Videos: {len(videos)}\n"
                    f"ğŸ“¦ Total: {len(media_items)}\n\n"
                    f"ğŸ“‚ Files downloaded to:\n`{download_dir}`\n\n"
                    f"ğŸ“‹ Sending files..."
                )
            
            await status_message.edit_text(summary, parse_mode=ParseMode.MARKDOWN)
            
            # Send CSV
            if csv_file and os.path.exists(csv_file):
                caption = "ğŸ“Š **Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· CSV**\nØ¬Ù…ÙŠØ¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ÙˆØ§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©" if lang == 'ar' else "ğŸ“Š **Media List CSV**\nAll media URLs and local paths"
                with open(csv_file, 'rb') as f:
                    await message.reply_document(document=f, filename=os.path.basename(csv_file), caption=caption)
            
            # Send media files (limit 10)
            sent_count = 0
            max_send = 10
            
            for item in media_items[:max_send]:
                if item.get('status') == 'downloaded' and item.get('local_path'):
                    filepath = item['local_path']
                    if os.path.exists(filepath):
                        try:
                            with open(filepath, 'rb') as f:
                                if item['type'] == 'image':
                                    await message.reply_photo(photo=f, caption=f"ğŸ–¼ {item['filename']}")
                                elif item['type'] == 'video':
                                    await message.reply_video(video=f, caption=f"ğŸ¥ {item['filename']}")
                            sent_count += 1
                        except Exception as e:
                            logger.error(f"Failed to send {filepath}: {str(e)}")
            
            # Final message
            if lang == 'ar':
                final_msg = f"âœ… **Ù…ÙƒØªÙ…Ù„!**\n\nğŸ“¤ ØªÙ… Ø¥Ø±Ø³Ø§Ù„ {sent_count} Ù…Ù„Ù ÙˆØ³Ø§Ø¦Ø·\n"
                if len(media_items) > max_send:
                    final_msg += f"\nâš ï¸ Ø¹Ø±Ø¶ Ø£ÙˆÙ„ {max_send} Ù…Ù† {len(media_items)} Ù…Ù„Ù ÙÙ‚Ø·\n"
                    final_msg += f"ğŸ“‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ:\n`{download_dir}`\n"
                final_msg += f"\nğŸ’¡ Ø±Ø§Ø¬Ø¹ CSV Ù„Ø¬Ù…ÙŠØ¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙˆØ³Ø§Ø¦Ø·"
            else:
                final_msg = f"âœ… **Complete!**\n\nğŸ“¤ Sent {sent_count} media files\n"
                if len(media_items) > max_send:
                    final_msg += f"\nâš ï¸ Only showing first {max_send} of {len(media_items)} files\n"
                    final_msg += f"ğŸ“‚ All files saved in:\n`{download_dir}`\n"
                final_msg += f"\nğŸ’¡ Check the CSV for all media URLs"
            
            await message.reply_text(final_msg, parse_mode=ParseMode.MARKDOWN)
            
            if hasattr(twitter_scraper, 'close'):
                twitter_scraper.close()
            
        except Exception as e:
            logger.error(f"Twitter scraping error: {str(e)}")
            import traceback
            logger.debug(traceback.format_exc())
            
            if lang == 'ar':
                error_msg = (
                    f"âŒ **ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÙŠØªØ±!**\n\n"
                    f"`{str(e)}`\n\n"
                    f"ğŸ’¡ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ø¨Ø³Ø¨Ø¨:\n"
                    f"â€¢ Gallery-dl ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ Ù„Ø§ ÙŠØ¹Ù…Ù„\n"
                    f"â€¢ ÙØ§ÙŠØ±ÙÙˆÙƒØ³ ØºÙŠØ± Ù…ÙØªÙˆØ­ Ø£Ùˆ Ù…Ù„ÙØ§Øª ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©\n"
                    f"â€¢ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ø®Ø§ØµØ© Ø£Ùˆ Ù…Ø­Ø°ÙˆÙØ©\n"
                    f"â€¢ Ù…Ø´Ø§ÙƒÙ„ Ø§ØªØµØ§Ù„ Ø§Ù„Ø´Ø¨ÙƒØ©\n\n"
                    f"ğŸ“– Ø±Ø§Ø¬Ø¹ `TWITTER_COOKIES_GUIDE.md` Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯\n\n"
                    f"Ø¬Ø±Ø¨ ØªØºØ±ÙŠØ¯Ø© Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ø§Ù‹."
                )
            else:
                error_msg = (
                    f"âŒ **Twitter scraping failed!**\n\n"
                    f"`{str(e)}`\n\n"
                    f"ğŸ’¡ This might be because:\n"
                    f"â€¢ Gallery-dl not found or not working\n"
                    f"â€¢ Firefox not open or cookies unavailable\n"
                    f"â€¢ Tweet is private or deleted\n"
                    f"â€¢ Network connectivity issues\n\n"
                    f"ğŸ“– See `TWITTER_COOKIES_GUIDE.md` for setup help\n\n"
                    f"Try a different tweet or try again later."
                )
            
            await status_message.edit_text(error_msg, parse_mode=ParseMode.MARKDOWN)
            
            try:
                if hasattr(twitter_scraper, 'close'):
                    twitter_scraper.close()
            except:
                pass
    
    async def error_handler(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Handle errors"""
        print(f"Update {update} caused error {context.error}")
        if update and update.effective_message:
            lang = self.get_user_language(update)
            error_msg = "âŒ Ø­Ø¯Ø« Ø®Ø·Ø£. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ø§Ù‹." if lang == 'ar' else "âŒ An error occurred. Please try again later."
            await update.effective_message.reply_text(error_msg)
    
    def run(self):
        """Run the bot"""
        print("=" * 60)
        print("ğŸ¤– BILINGUAL WEB SCRAPER BOT (English & Arabic)")
        print("ğŸŒ Ø¨ÙˆØª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ© (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)")
        print("=" * 60)
        print("\nâœ… Starting bot...")
        print("âœ… Ø¬Ø§Ø±Ù Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª...")
        
        application = Application.builder().token(BOT_TOKEN).build()
        
        # Register handlers
        application.add_handler(CommandHandler("start", self.start_command))
        application.add_handler(CommandHandler("help", self.help_command))
        application.add_handler(CommandHandler("scrape", self.scrape_command))
        application.add_handler(CommandHandler("lang", self.lang_command))
        application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_message))
        application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND & filters.UpdateType.EDITED_MESSAGE, self.handle_message))
        application.add_error_handler(self.error_handler)
        
        print("âœ… Bot is ready! | Ø§Ù„Ø¨ÙˆØª Ø¬Ø§Ù‡Ø²!")
        print("\nBot Commands | Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ø¨ÙˆØª:")
        print("  /start - Welcome | ØªØ±Ø­ÙŠØ¨")
        print("  /help - Help | Ù…Ø³Ø§Ø¹Ø¯Ø©")
        print("  /scrape <URL> - Scrape | Ø§Ø³ØªØ®Ø±Ø§Ø¬")
        print("  /lang - Change language | ØªØºÙŠÙŠØ± Ø§Ù„Ù„ØºØ©")
        print("\nOr just send any URL! | Ø£Ùˆ ÙÙ‚Ø· Ø£Ø±Ø³Ù„ Ø£ÙŠ Ø±Ø§Ø¨Ø·!")
        print("\n" + "=" * 60)
        print("ğŸš€ Bot is running... | Ø§Ù„Ø¨ÙˆØª ÙŠØ¹Ù…Ù„...")
        print("Press Ctrl+C to stop | Ø§Ø¶ØºØ· Ctrl+C Ù„Ù„Ø¥ÙŠÙ‚Ø§Ù")
        print("=" * 60 + "\n")
        
        application.run_polling(allowed_updates=Update.ALL_TYPES)

def main():
    bot = BilingualScraperBot()
    bot.run()

if __name__ == '__main__':
    main()
